{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семинар 1 Индекс\n",
    "\n",
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### работа с файлами и папками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "filepath = os.path.join(curr_dir, 'test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### os.path  \n",
    "путь до файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# возвращает полный путь до папки/файла по имени файла / папки\n",
    "print(os.path.abspath(filepath))\n",
    "\n",
    "\n",
    "# возвращает имя файла / папки по полному ти до него\n",
    "print(os.path.basename(filepath))\n",
    "\n",
    "\n",
    "# проверить существование директории - True / False\n",
    "print(os.path.exists(curr_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### os.listdir  \n",
    "возвращает список файлов в данной директории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(curr_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При обходе файлов не забывайте исключать системные директории, такие как .DS_Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### os.walk\n",
    "root - начальная директория  \n",
    "dirs - список поддиректорий (папок)   \n",
    "files - список файлов в этих поддиректориях  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(curr_dir):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __os.walk__ возвращает генератор, это значит, что получить его элементы можно только проитерировавшись по нему  \n",
    "но его легко можно превратить в list и увидеть все его значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(os.walk(curr_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### чтение файла "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = 'test.txt'\n",
    "\n",
    "\n",
    "# одним массивом  \n",
    "with open(fpath, 'r') as f:  \n",
    "    text = f.read() \n",
    "\n",
    "    \n",
    "#по строкам, в конце каждой строки \\n  \n",
    "with open(fpath, 'r') as f:   \n",
    "    text = f.readlines() \n",
    "\n",
    "    \n",
    "#по строкам, без \\n   \n",
    "with open(fpath, 'r') as f:   \n",
    "    text = f.read().splitlines() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напоминание про enumerate:    \n",
    "> При итерации по списку вы можете помимо самого элемента получить его порядковый номер    \n",
    "``` for i, element in enumerate(your_list): ...  ```    \n",
    "Иногда для получения элемента делают так -  ``` your_list[i] ```, не надо так"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    " \n",
    "# инициализируем\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# составляем корпус документов\n",
    "corpus = [\n",
    "  'слово1 слово2 слово3',\n",
    "  'слово2 слово3',\n",
    "  'слово1 слово2 слово1',\n",
    "  'слово4'\n",
    "]\n",
    "\n",
    "# считаем\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X)\n",
    "\n",
    "# получится следующая структура:\n",
    "#        | слово1 | слово2 | слово3 | слово4\n",
    "# текст1 |   1    |    1   |   1    |   0\n",
    "# текст2 |   0    |    1   |   1    |   0\n",
    "# текст3 |   2    |    1   |   0    |   0\n",
    "# текст4 |   0    |    0   |   0    |   1\n",
    " \n",
    "# чтобы получить сгенерированный словарь, из приведенной структуры CountVectorizer\n",
    "# порядок совпадает с матрицей\n",
    "print(vectorizer.get_feature_names())  # ['слово1', 'слово2', 'слово3', 'слово4']\n",
    " \n",
    "# чтобы узнать индекс токена в словаре\n",
    "print(vectorizer.vocabulary_.get('слово3')) # вернет 2\n",
    " \n",
    "# показать матрицу\n",
    "X.toarray()\n",
    " \n",
    "# теперь можно быстро подсчитать вектор для нового документа\n",
    "print(vectorizer.transform([\"слово1 слово4 слово4\"]))  # результат [[1 0 0 2]]\n",
    " \n",
    "# чтобы узнать количественное вхождение каждого слова:\n",
    "matrix_freq = np.asarray(X.sum(axis=0)).ravel()\n",
    "print(matrix_freq)\n",
    "final_matrix = np.array([np.array(vectorizer.get_feature_names()), matrix_freq])\n",
    "print(final_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Индекс \n",
    "\n",
    "Сам по себе индекс - это просто формат хранения данных, он не может осуществлять поиск. Для этого необходимо добавить к нему определенную метрику. Это может быть что-то простое типа булева поиска, а может быть что-то более специфическое или кастомное под задачу.\n",
    "\n",
    "Давайте посмотрим, что полезного можно вытащить из самого индекса.    \n",
    "По сути, индекс - это информация о частоте встречаемости слова в каждом документе.   \n",
    "Из этого можно понять, например:\n",
    "1. какое слово является самым часто употребимым / редким\n",
    "2. какие слова встречаются всегда вместе - так можно парсить твиттер, fb, форумы и отлавливать новые устойчивые выражения в речи\n",
    "3. как эти документы кластеризуются по N тематикам согласно словам, которые в них упоминаются "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Задача__: \n",
    "\n",
    "**Data:** Коллекция субтитров сезонов Друзей. Одна серия - один документ.\n",
    "\n",
    "**To do:** \n",
    "\n",
    "**1 Создайте индекс этой базы в формате json и в формате матрицы Term-Document**\n",
    "\n",
    "Компоненты вашей реализации:\n",
    "    - функция препроцессинга данных\n",
    "    - функция индексирования данных\n",
    "\n",
    "**2 С помощью обратного индекса в формате Term-Document посчитайте:** \n",
    "\n",
    "\n",
    "a) какое слово является самым частотным\n",
    "\n",
    "b) какое самым редким\n",
    "\n",
    "c) какой набор слов есть во всех документах коллекции\n",
    "\n",
    "d) кто из главных героев статистически самый популярный? \n",
    "\n",
    "\n",
    "\n",
    "[download_friends_corpus](https://yadi.sk/d/4wmU7R8JL-k_RA?w=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напоминание про defaultdict: \n",
    "> Если ваш код предполагает использование multiple values словаря, рекомендую использовать ``` collections.defaultdict ```                          \n",
    "> Так можно избежать конструкции ``` dict.setdefault(key, default=None) ```\n",
    "\n",
    "> ```from collections import defaultdict\n",
    "d = defaultdict(list)\n",
    "d['example'].append('example1')\n",
    "d['example'].append('example2')```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### _check : в коллекции должно быть около 165 файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import pymorphy2\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция препроцессинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "morphy_norm = pymorphy2.MorphAnalyzer()\n",
    "stop_words = stopwords.words('russian')\n",
    "\n",
    "def my_preprocessor(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[\\'\\:\\;\\»\\«\\>\\—]\"”\\,\\!\\?\\.\\-\\(\\)\\[\\]', ' ', str(text).rstrip(\"']\"))\n",
    "    text = text.replace('\\ufeff', '')\n",
    "    words = [i for i in text.split() if i not in stop_words]\n",
    "    words = [morphy_norm.parse(i)[0].normal_form for i in words]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создадим обратный индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25197"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serial_dir = 'friends-data'\n",
    "documents_array = []\n",
    "file_names = []\n",
    "\n",
    "for root, dirs, files in os.walk(serial_dir):\n",
    "    for name in files:\n",
    "        file_names.append(os.path.join(root, name))\n",
    "            \n",
    "file_names = sorted(file_names)\n",
    "for i in file_names:\n",
    "    with open(i, 'r') as f:  \n",
    "        documents_array.append(f.read())\n",
    "file_names = [i.split('/')[-1] for i in file_names]\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=my_preprocessor)\n",
    "count_vector = vectorizer.fit_transform( documents_array )\n",
    "\n",
    "X = vectorizer.fit_transform(documents_array)\n",
    "X.toarray()\n",
    "\n",
    "matrix_freq = np.asarray(X.sum(axis=0)).ravel()\n",
    "final_matrix = np.array([np.array(vectorizer.get_feature_names()), matrix_freq])\n",
    "vec_of_words = vectorizer.get_feature_names()\n",
    "\n",
    "zipped_word_freq = [(int(i[0]), i[1]) for i in list( zip(final_matrix[1], final_matrix[0]) )]\n",
    "len(zipped_word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразуем в json вида { 'слово': [ в 1 документе, во 2 документе, ...], ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25197"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_dict = dict()\n",
    "for word in vec_of_words:\n",
    "    json_dict[word] = []\n",
    "\n",
    "for series_ind, vec in enumerate(X):\n",
    "    for word_ind, num in enumerate(vec.toarray()[0]):\n",
    "#         if num != 0:\n",
    "#             json_dict[vec_of_words[word_ind]].append( (file_names[series_ind], num) )\n",
    "        json_dict[vec_of_words[word_ind]].append( int(num) )\n",
    "len(json_dict) # количество слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) 10 самых частовстречаемых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_freq = np.asarray(X.sum(axis=0)).ravel()\n",
    "final_matrix = np.array([np.array(vectorizer.get_feature_names()), matrix_freq])\n",
    "\n",
    "zipped_word_freq = [(int(i[0]), i[1]) for i in list( zip(final_matrix[1], final_matrix[0]) )]\n",
    "zipped_word_freq = sorted(zipped_word_freq, key=lambda x: int(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6519, 'это'),\n",
       " (2537, 'что'),\n",
       " (2479, 'нет'),\n",
       " (2298, 'да'),\n",
       " (2112, 'ты'),\n",
       " (1715, 'мочь'),\n",
       " (1584, 'то'),\n",
       " (1560, 'хотеть'),\n",
       " (1459, 'мой'),\n",
       " (1268, 'весь')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipped_word_freq[-10:][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) 10 первых слов, встречающиеся по 1 разу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, '03815')\n",
      "(1, '0fps')\n",
      "(1, '101')\n",
      "(1, '102')\n",
      "(1, '110')\n",
      "(1, '1100')\n",
      "(1, '112')\n",
      "(1, '11б')\n",
      "(1, '1250')\n",
      "(1, '128')\n"
     ]
    }
   ],
   "source": [
    "counter = 10\n",
    "for i in zipped_word_freq:\n",
    "    if i[0] > 1 or counter == 0:\n",
    "        break\n",
    "    else:\n",
    "        counter -= 1\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Набор слов, которые есть во всех документах коллекции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "быть\n",
      "да\n",
      "мой\n",
      "нет\n",
      "просто\n",
      "то\n",
      "ты\n",
      "хотеть\n",
      "что\n",
      "это\n"
     ]
    }
   ],
   "source": [
    "for k, vec in json_dict.items():\n",
    "    if np.count_nonzero(vec) == len(vec):\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Самый популярный персонаж"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(856, 'росс'),\n",
       " (575, 'фиби'),\n",
       " (559, 'моника'),\n",
       " (386, 'чендлер'),\n",
       " (237, 'рэйчел'),\n",
       " (222, 'джоуя')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persons = [\"рэйчел\", \"чендлер\", \"фиби\", \"джоуя\", \"росс\", \"моника\"]\n",
    "name_freq = []\n",
    "for freq, name in zipped_word_freq:\n",
    "    for i in persons:\n",
    "        if name == i:\n",
    "            name_freq.append((freq, name))\n",
    "sorted(name_freq, key=lambda x: x[0])[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Трансформация в pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# словарь в json\n",
    "with open(\"output.json\", \"w\") as f:\n",
    "    json.dump(json_dict, f)\n",
    "\n",
    "# словать в пикл\n",
    "with open('output_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(json_dict, f)\n",
    "    \n",
    "#матрицу в пикл\n",
    "with open('output_mtx.pickle', 'wb') as f:\n",
    "    pickle.dump(X, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
