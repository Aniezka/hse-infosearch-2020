{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 3  NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Задача 1__:\n",
    "\n",
    "Реализуйте 2 функции препроцессинга:\n",
    "\n",
    "- Удалить именованные сущности с помощью natasha (https://github.com/natasha/yargy)\n",
    "- Удалить именованные сущности с помощью deepmipt (https://github.com/deepmipt/ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    NewsEmbedding,\n",
    "    NewsNERTagger,\n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "    DatesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-01 22:29:19.382 INFO in 'deeppavlov.download'['download'] at line 132: Skipped http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_v1.tar.gz download because of matching hashes\n",
      "2020-10-01 22:29:24.606 INFO in 'deeppavlov.download'['download'] at line 132: Skipped http://files.deeppavlov.ai/deeppavlov_data/ner_rus_bert_v1.tar.gz download because of matching hashes\n",
      "2020-10-01 22:29:25.112 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /Users/Anna/.deeppavlov/models/ner_rus_bert/tag.dict]\n",
      "2020-10-01 22:29:52.145 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /Users/Anna/.deeppavlov/models/ner_rus_bert/model]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/Anna/.deeppavlov/models/ner_rus_bert/model\n"
     ]
    }
   ],
   "source": [
    "from natasha import DatesExtractor\n",
    "import numpy as np\n",
    "from deeppavlov import configs, build_model\n",
    "\n",
    "ner_model = build_model(configs.ner.ner_rus_bert, download=True)\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "segmenter = Segmenter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from razdel import tokenize, sentenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def preprocess_with_natasha(text: str) -> str:\n",
    "    text = str(text)\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_ner(ner_tagger)\n",
    "    for ner in doc.spans:\n",
    "        text = text.replace(ner.text, \"\")\n",
    "    return \" \".join(text.split())\n",
    "#     return my_preprocess(text)\n",
    "\n",
    "def preprocess_with_deepmipt(text: str) -> str:\n",
    "    text = str(text)\n",
    "    text_splitted = sentenize(text)\n",
    "    for sent in text_splitted:\n",
    "        sent = str(sent.text)\n",
    "        if len(sent) == 0:\n",
    "            break\n",
    "        sent_buf = list(tokenize(sent))\n",
    "        if len(sent_buf) >= 300:\n",
    "            sent = \" \".join([str(str_buf) for str_buf in sent_buf[:300]])\n",
    "#         print(len(sent))\n",
    "#         print(sent)\n",
    "        res = ner_model([sent])\n",
    "        for m in zip(res[0][0], res[1][0]):\n",
    "            if m[1][0] == \"B\" or m[1][0] == \"I\":\n",
    "                text = text.replace(m[0], \"\")\n",
    "            \n",
    "    return \" \".join(text.split())\n",
    "#     return my_preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Задача 2__:    \n",
    "На предыдущем занятии вы реализовывали функции поиска ближайших ответов на запросы через TF-IDF и BM25. \n",
    "Сравните качество нахождения верного ответа для обоих методов в трех случаях:\n",
    "- с функцией ```preprocess_with_natasha```\n",
    "- с функцией ```preprocess_with_deepmipt```\n",
    "- без препроцессинга\n",
    "\n",
    "Для измерения качества используйте метрику accuracy. Считаем, что ответ верный, если он входит в топ-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_df = pd.read_excel(\"answers_base.xlsx\")\n",
    "questions_df = pd.read_excel(\"queries_base.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natasha + TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Anna/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n",
      "100%|██████████| 1652/1652 [00:15<00:00, 108.55it/s]\n",
      "100%|██████████| 690/690 [00:05<00:00, 125.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1652, 13065)\n",
      "X_test.shape: (690, 13065)\n"
     ]
    }
   ],
   "source": [
    "answers_df.rename(columns={'Текст вопросов': 'text', 'Номер связки': 'join_num'}, inplace=True)\n",
    "questions_df.rename(columns={'Текст вопроса': 'text', 'Номер связки\\n': 'join_num'}, inplace=True)\n",
    "\n",
    "train, test_quest = train_test_split(questions_df, test_size=0.3)\n",
    "\n",
    "train_quest = pd.concat([answers_df, train])\n",
    "\n",
    "train_quest_processed = []\n",
    "test_quest_processed = []\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# preprocess train / test\n",
    "for quest in tqdm(train_quest['text']):\n",
    "    train_quest_processed.append(preprocess_with_natasha(quest))\n",
    "for quest in tqdm(test_quest['text']):\n",
    "    test_quest_processed.append(preprocess_with_natasha(quest))\n",
    "\n",
    "vectorizer.fit(train_quest_processed + test_quest_processed)\n",
    "# train matrix\n",
    "X_train = vectorizer.transform(train_quest_processed)\n",
    "# test natrix\n",
    "X_test = vectorizer.transform(test_quest_processed)\n",
    "\n",
    "print(\"X_train.shape: \" + str(X_train.shape))\n",
    "print(\"X_test.shape: \" + str(X_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy with natasha and TF-IDF: 0.5130434782608696'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating = X_train.dot(X_test.T).argmax(axis=0)\n",
    "rating = np.array(rating)[0]\n",
    "count = 0\n",
    "for ind_test, pred in enumerate(rating):\n",
    "    if math.isnan(test_quest.iloc[ind_test].join_num) or math.isnan(train_quest.iloc[pred].join_num):\n",
    "        continue\n",
    "\n",
    "    if int(test_quest.iloc[ind_test].join_num) == int(train_quest.iloc[pred].join_num):\n",
    "        count += 1\n",
    "\n",
    "\"Accuracy with natasha and TF-IDF: \" + str(count / len(rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepmipt + TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1652/1652 [17:43<00:00,  1.55it/s] \n",
      "100%|██████████| 690/690 [07:32<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1652, 13934)\n",
      "X_test.shape: (690, 13934)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Accuracy with deepmipt and TF-IDF: 0.4927536231884058'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_quest_processed = []\n",
    "test_quest_processed = []\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# preprocess train / test\n",
    "for quest in tqdm(train_quest['text']):\n",
    "    train_quest_processed.append(preprocess_with_deepmipt(quest))\n",
    "for quest in tqdm(test_quest['text']):\n",
    "    test_quest_processed.append(preprocess_with_deepmipt(quest))\n",
    "\n",
    "vectorizer.fit(train_quest_processed + test_quest_processed)\n",
    "# train matrix\n",
    "X_train = vectorizer.transform(train_quest_processed)\n",
    "# test natrix\n",
    "X_test = vectorizer.transform(test_quest_processed)\n",
    "\n",
    "print(\"X_train.shape: \" + str(X_train.shape))\n",
    "print(\"X_test.shape: \" + str(X_test.shape))\n",
    "\n",
    "rating = X_train.dot(X_test.T).argmax(axis=0)\n",
    "rating = np.array(rating)[0]\n",
    "count = 0\n",
    "for ind_test, pred in enumerate(rating):\n",
    "    if math.isnan(test_quest.iloc[ind_test].join_num) or math.isnan(train_quest.iloc[pred].join_num):\n",
    "        continue\n",
    "\n",
    "    if int(test_quest.iloc[ind_test].join_num) == int(train_quest.iloc[pred].join_num):\n",
    "        count += 1\n",
    "\n",
    "\"Accuracy with deepmipt and TF-IDF: \" + str(count / len(rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without preprocessing + TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1652/1652 [00:00<00:00, 15825.10it/s]\n",
      "100%|██████████| 690/690 [00:00<00:00, 773608.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1652, 14972)\n",
      "X_test.shape: (690, 14972)\n"
     ]
    }
   ],
   "source": [
    "train_quest_processed = []\n",
    "test_quest_processed = []\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# preprocess train / test\n",
    "for quest in tqdm(train_quest['text']):\n",
    "    train_quest_processed.append(str(quest))\n",
    "for quest in tqdm(test_quest['text']):\n",
    "    test_quest_processed.append(str(quest))\n",
    "\n",
    "vectorizer.fit(train_quest_processed + test_quest_processed)\n",
    "# train matrix\n",
    "X_train = vectorizer.transform(train_quest_processed)\n",
    "# test natrix\n",
    "X_test = vectorizer.transform(test_quest_processed)\n",
    "\n",
    "print(\"X_train.shape: \" + str(X_train.shape))\n",
    "print(\"X_test.shape: \" + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy without any preprocessing and TF-IDF: 0.5217391304347826'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating = X_train.dot(X_test.T).argmax(axis=0)\n",
    "rating = np.array(rating)[0]\n",
    "count = 0\n",
    "for ind_test, pred in enumerate(rating):\n",
    "    if math.isnan(test_quest.iloc[ind_test].join_num) or math.isnan(train_quest.iloc[pred].join_num):\n",
    "        continue\n",
    "\n",
    "    if int(test_quest.iloc[ind_test].join_num) == int(train_quest.iloc[pred].join_num):\n",
    "        count += 1\n",
    "\n",
    "\"Accuracy without any preprocessing and TF-IDF: \" + str(count / len(rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 векторизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from razdel import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def get_inverse_dict(mat):\n",
    "    d = dict()\n",
    "    mat = mat.toarray()\n",
    "    for ind, word in enumerate(vectorizer.get_feature_names()):\n",
    "        d[word] = [int(sum(mat[:, ind]))]\n",
    "        for ind_j, doc_ind in enumerate(mat[:, ind].tolist()):\n",
    "            if doc_ind != 0:\n",
    "                d[word].append(doc_ind)\n",
    "        d[word].append(ind)\n",
    "    return d\n",
    "\n",
    "\n",
    "def vec_bm25(doc, dict_words):\n",
    "    vec = np.zeros((1, len(dict_words)))\n",
    "    doc = preprocess_with_natasha(doc)\n",
    "    for word in doc.split(\" \"):\n",
    "        if word in dict_words.keys():\n",
    "            vec[0, dict_words[word][-1]] = 1\n",
    "    return vec\n",
    "\n",
    "\n",
    "def bm25_vectorizer(tf_val, len_d, corpus_len, nq):\n",
    "    k = 2.0\n",
    "    b = 0.75\n",
    "    IDF = np.log((corpus_len-nq+0.5) / (nq+0.5))\n",
    "    TF = (tf_val * (k+1)) / (tf_val + k * (1-b+b*(len_d / avrdl)))\n",
    "    return TF * IDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without preprocessing + BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1652/1652 [00:00<00:00, 640801.83it/s]\n",
      "100%|██████████| 690/690 [00:00<00:00, 547186.57it/s]\n"
     ]
    }
   ],
   "source": [
    "train_quest_processed = []\n",
    "test_quest_processed = []\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# preprocess train / test\n",
    "for quest in tqdm(train_quest['text']):\n",
    "    train_quest_processed.append(str(quest))\n",
    "for quest in tqdm(test_quest['text']):\n",
    "    test_quest_processed.append(str(quest))\n",
    "\n",
    "corpus_len = len(train_quest_processed)\n",
    "avrdl = sum([len(i.split(\" \")) for i in train_quest_processed]) / corpus_len\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_quest_processed)\n",
    "\n",
    "inverse_dict = get_inverse_dict(X_train)\n",
    "\n",
    "mat = np.zeros((corpus_len, len(inverse_dict)))\n",
    "\n",
    "for ind, doc in enumerate(train_quest_processed):\n",
    "    tokens = doc.split(\" \")\n",
    "    tf_values = collections.Counter(tokens)\n",
    "    len_d = len(tokens)\n",
    "    for word in tokens:\n",
    "        if word not in inverse_dict.keys():\n",
    "            continue\n",
    "#         print(\"Ffff\")\n",
    "        mat[ind, inverse_dict[word][-1]] = bm25_vectorizer(tf_values[word],\n",
    "                                                         len_d,\n",
    "                                                         corpus_len,\n",
    "                                                         len(inverse_dict[word]) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy without any preprocessing and BM25: 0.5028985507246376'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vecs = []\n",
    "for i in test_quest.text.values:\n",
    "    test_vecs.append(vec_bm25(i, inverse_dict)[0])\n",
    "test_vecs = np.array(test_vecs)\n",
    "\n",
    "rating = mat.dot(test_vecs.T).argmax(axis=0)\n",
    "rating = np.array(rating)\n",
    "\n",
    "count = 0\n",
    "for ind_test, pred in enumerate(rating):\n",
    "    if math.isnan(test_quest.iloc[ind_test].join_num) or math.isnan(train_quest.iloc[pred].join_num):\n",
    "        continue\n",
    "\n",
    "    if int(test_quest.iloc[ind_test].join_num) == int(train_quest.iloc[pred].join_num):\n",
    "        count += 1\n",
    "\n",
    "\"Accuracy without any preprocessing and BM25: \" + str(count / len(rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natasha + BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1652/1652 [00:11<00:00, 144.94it/s]\n",
      "100%|██████████| 690/690 [00:16<00:00, 41.98it/s] \n"
     ]
    }
   ],
   "source": [
    "train_quest_processed = []\n",
    "test_quest_processed = []\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# preprocess train / test\n",
    "for quest in tqdm(train_quest['text']):\n",
    "    train_quest_processed.append(preprocess_with_natasha(quest))\n",
    "for quest in tqdm(test_quest['text']):\n",
    "    test_quest_processed.append(preprocess_with_natasha(quest))\n",
    "\n",
    "corpus_len = len(train_quest_processed)\n",
    "avrdl = sum([len(i.split(\" \")) for i in train_quest_processed]) / corpus_len\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_quest_processed)\n",
    "\n",
    "inverse_dict = get_inverse_dict(X_train)\n",
    "\n",
    "mat = np.zeros((corpus_len, len(inverse_dict)))\n",
    "\n",
    "for ind, doc in enumerate(train_quest_processed):\n",
    "    tokens = doc.split(\" \")\n",
    "    tf_values = collections.Counter(tokens)\n",
    "    len_d = len(tokens)\n",
    "    for word in tokens:\n",
    "        if word not in inverse_dict.keys():\n",
    "            continue\n",
    "#         print(\"Ffff\")\n",
    "        mat[ind, inverse_dict[word][-1]] = bm25_vectorizer(tf_values[word],\n",
    "                                                         len_d,\n",
    "                                                         corpus_len,\n",
    "                                                         len(inverse_dict[word]) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy with natasha and BM25: 0.5028985507246376'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vecs = []\n",
    "for i in test_quest.text.values:\n",
    "    test_vecs.append(vec_bm25(i, inverse_dict)[0])\n",
    "test_vecs = np.array(test_vecs)\n",
    "\n",
    "rating = mat.dot(test_vecs.T).argmax(axis=0)\n",
    "rating = np.array(rating)\n",
    "\n",
    "count = 0\n",
    "for ind_test, pred in enumerate(rating):\n",
    "    if math.isnan(test_quest.iloc[ind_test].join_num) or math.isnan(train_quest.iloc[pred].join_num):\n",
    "        continue\n",
    "\n",
    "    if int(test_quest.iloc[ind_test].join_num) == int(train_quest.iloc[pred].join_num):\n",
    "        count += 1\n",
    "\n",
    "\"Accuracy with natasha and BM25: \" + str(count / len(rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepmipt + BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1652/1652 [13:25<00:00,  2.05it/s] \n",
      "100%|██████████| 690/690 [05:17<00:00,  2.17it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-9e616688a4e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_quest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_num\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_quest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2086\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2087\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2088\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2090\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "strtrain_quest_processed = []\n",
    "test_quest_processed = []\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# preprocess train / test\n",
    "for quest in tqdm(train_quest['text']):\n",
    "    train_quest_processed.append(preprocess_with_deepmipt(quest))\n",
    "for quest in tqdm(test_quest['text']):\n",
    "    test_quest_processed.append(preprocess_with_deepmipt(quest))\n",
    "\n",
    "corpus_len = len(train_quest_processed)\n",
    "avrdl = sum([len(i.split(\" \")) for i in train_quest_processed]) / corpus_len\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_quest_processed)\n",
    "\n",
    "inverse_dict = get_inverse_dict(X_train)\n",
    "\n",
    "mat = np.zeros((corpus_len, len(inverse_dict)))\n",
    "\n",
    "for ind, doc in enumerate(train_quest_processed):\n",
    "    tokens = doc.split(\" \")\n",
    "    tf_values = collections.Counter(tokens)\n",
    "    len_d = len(tokens)\n",
    "    for word in tokens:\n",
    "        if word not in inverse_dict.keys():\n",
    "            continue\n",
    "#         print(\"Ffff\")\n",
    "        mat[ind, inverse_dict[word][-1]] = bm25_vectorizer(tf_values[word],\n",
    "                                                         len_d,\n",
    "                                                         corpus_len,\n",
    "                                                         len(inverse_dict[word]) - 1)\n",
    "    \n",
    "test_vecs = []\n",
    "for i in test_quest.text.values:\n",
    "    test_vecs.append(vec_bm25(i, inverse_dict)[0])\n",
    "test_vecs = np.array(test_vecs)\n",
    "\n",
    "rating = mat.dot(test_vecs.T).argmax(axis=0)\n",
    "rating = np.array(rating)\n",
    "\n",
    "count = 0\n",
    "for ind_test, pred in enumerate(rating):\n",
    "    if math.isnan(test_quest.iloc[ind_test].join_num) or math.isnan(train_quest.iloc[pred].join_num):\n",
    "        continue\n",
    "\n",
    "    if int(test_quest.iloc[ind_test].join_num) == int(train_quest.iloc[pred].join_num):\n",
    "        count += 1\n",
    "\n",
    "\"Accuracy with deepmipt and BM25: \" + str(count / len(rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Задача 3__:    \n",
    "Улучшить правила в natasha. Написать правила, которые ловят даты в следующих примерах и пересчитать статистику из Задачи 2:\n",
    "- Уехал 8-9 ноября в Сочи\n",
    "- Уезжаю 5 числа                           \n",
    "- 20го сентября заболел\n",
    "\n",
    "Пример можно посмотреть тут: https://github.com/natasha/yargy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 19) ['8', 'января', '2014', 'года']\n",
      "[21, 36) ['15', 'июня', '2001', 'г', '.']\n",
      "[38, 53) ['31', 'февраля', '2018']\n",
      "[60, 70) ['8', '-', '9', 'ноября']\n",
      "[85, 92) ['5', 'числа']\n",
      "[93, 106) ['20', 'го', 'сентября']\n"
     ]
    }
   ],
   "source": [
    "from yargy import or_\n",
    "from yargy.predicates import caseless, normalized, dictionary\n",
    "from yargy import rule, and_, Parser\n",
    "from yargy.predicates import gte, lte\n",
    "\n",
    "\n",
    "DAY = and_(\n",
    "    gte(1),\n",
    "    lte(31)\n",
    ")\n",
    "MONTH = and_(\n",
    "    gte(1),\n",
    "    lte(12)\n",
    ")\n",
    "YEAR = and_(\n",
    "    gte(1),\n",
    "    lte(2018)\n",
    ")\n",
    "DATE = rule(\n",
    "    YEAR,\n",
    "    '-',\n",
    "    MONTH,\n",
    "    '-',\n",
    "    DAY\n",
    ")\n",
    "\n",
    "\n",
    "MONTHS = {\n",
    "    'январь',\n",
    "    'февраль',\n",
    "    'март',\n",
    "    'апрель',\n",
    "    'мая',\n",
    "    'июнь',\n",
    "    'июль',\n",
    "    'август',\n",
    "    'сентябрь',\n",
    "    'октябрь',\n",
    "    'ноябрь',\n",
    "    'декабрь'\n",
    "}\n",
    "MONTH_NAME = dictionary(MONTHS)\n",
    "YEAR_WORDS = or_(\n",
    "    rule(caseless('г'), '.'),\n",
    "    rule(normalized('год')),\n",
    "    rule(normalized('число')),\n",
    "    rule(caseless('числа')),\n",
    "    rule(caseless('го'))\n",
    ")\n",
    "\n",
    "DATE = or_(\n",
    "    rule(\n",
    "        YEAR,\n",
    "        '-',\n",
    "        MONTH,\n",
    "        '-',\n",
    "        DAY\n",
    "    ),\n",
    "    rule(\n",
    "        DAY,\n",
    "        MONTH_NAME,\n",
    "        YEAR,\n",
    "        YEAR_WORDS.optional()\n",
    "    ),\n",
    "    rule(\n",
    "        DAY,\n",
    "        \"-\",\n",
    "        DAY,\n",
    "        MONTH_NAME,\n",
    "    ),\n",
    "    rule(\n",
    "        DAY,\n",
    "        YEAR_WORDS.optional(),\n",
    "        MONTH_NAME.optional()\n",
    "    )\n",
    ")\n",
    "\n",
    "parser = Parser(DATE)\n",
    "text = '''\n",
    "8 января 2014 года, 15 июня 2001 г.,\n",
    "31 февраля 2018\n",
    "Уехал 8-9 ноября в Сочи\n",
    "Уезжаю 5 числа\n",
    "20го сентября заболел'''\n",
    "for match in parser.findall(text):\n",
    "    print(match.span, [_.value for _ in match.tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser(DATE)\n",
    "\n",
    "def preprocess_with_natasha_date(text: str) -> str:\n",
    "    text = str(text)\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_ner(ner_tagger)\n",
    "    for ner in doc.spans:\n",
    "        text = text.replace(ner.text, \"\")\n",
    "    for match in parser.findall(text):\n",
    "        for tok in match.tokens:\n",
    "            text = text.replace(tok.value, \"\")\n",
    "    return \" \".join(text.split())\n",
    "#     return my_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Anna/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n",
      "100%|██████████| 1652/1652 [00:26<00:00, 61.41it/s] \n",
      "100%|██████████| 690/690 [00:10<00:00, 68.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1652, 12945)\n",
      "X_test.shape: (690, 12945)\n"
     ]
    }
   ],
   "source": [
    "answers_df.rename(columns={'Текст вопросов': 'text', 'Номер связки': 'join_num'}, inplace=True)\n",
    "questions_df.rename(columns={'Текст вопроса': 'text', 'Номер связки\\n': 'join_num'}, inplace=True)\n",
    "\n",
    "train, test_quest = train_test_split(questions_df, test_size=0.3)\n",
    "\n",
    "train_quest = pd.concat([answers_df, train])\n",
    "\n",
    "train_quest_processed = []\n",
    "test_quest_processed = []\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# preprocess train / test\n",
    "for quest in tqdm(train_quest['text']):\n",
    "    train_quest_processed.append(preprocess_with_natasha_date(quest))\n",
    "for quest in tqdm(test_quest['text']):\n",
    "    test_quest_processed.append(preprocess_with_natasha_date(quest))\n",
    "\n",
    "vectorizer.fit(train_quest_processed + test_quest_processed)\n",
    "# train matrix\n",
    "X_train = vectorizer.transform(train_quest_processed)\n",
    "# test natrix\n",
    "X_test = vectorizer.transform(test_quest_processed)\n",
    "\n",
    "print(\"X_train.shape: \" + str(X_train.shape))\n",
    "print(\"X_test.shape: \" + str(X_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy with natasha and TF-IDF: 0.5362318840579711'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating = X_train.dot(X_test.T).argmax(axis=0)\n",
    "rating = np.array(rating)[0]\n",
    "count = 0\n",
    "for ind_test, pred in enumerate(rating):\n",
    "    if math.isnan(test_quest.iloc[ind_test].join_num) or math.isnan(train_quest.iloc[pred].join_num):\n",
    "        continue\n",
    "\n",
    "    if int(test_quest.iloc[ind_test].join_num) == int(train_quest.iloc[pred].join_num):\n",
    "        count += 1\n",
    "\n",
    "\"Accuracy with natasha and TF-IDF: \" + str(count / len(rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
