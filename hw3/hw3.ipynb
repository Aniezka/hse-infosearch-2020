{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 3  NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Задача 1__:\n",
    "\n",
    "Реализуйте 2 функции препроцессинга:\n",
    "\n",
    "- Удалить именованные сущности с помощью natasha (https://github.com/natasha/yargy)\n",
    "- Удалить именованные сущности с помощью deepmipt (https://github.com/deepmipt/ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    NewsEmbedding,\n",
    "    NewsNERTagger,\n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "    DatesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-02 13:06:47.607 INFO in 'deeppavlov.download'['download'] at line 132: Skipped http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_v1.tar.gz download because of matching hashes\n",
      "2020-10-02 13:06:51.772 INFO in 'deeppavlov.download'['download'] at line 132: Skipped http://files.deeppavlov.ai/deeppavlov_data/ner_rus_bert_v1.tar.gz download because of matching hashes\n",
      "[nltk_data] Downloading package punkt to /Users/Anna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/Anna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /Users/Anna/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /Users/Anna/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-02 13:07:07.295 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /Users/Anna/.deeppavlov/models/ner_rus_bert/tag.dict]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/deeppavlov/models/bert/bert_sequence_tagger.py:236: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/deeppavlov/models/bert/bert_sequence_tagger.py:314: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/bert_dp/modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/bert_dp/modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/bert_dp/modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
      "\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/deeppavlov/models/bert/bert_sequence_tagger.py:75: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/crf/python/ops/crf.py:213: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:131: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:131: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:94: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/deeppavlov/models/bert/bert_sequence_tagger.py:671: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/deeppavlov/models/bert/bert_sequence_tagger.py:244: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/deeppavlov/models/bert/bert_sequence_tagger.py:249: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-02 13:07:34.977 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /Users/Anna/.deeppavlov/models/ner_rus_bert/model]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Anna/opt/anaconda3/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /Users/Anna/.deeppavlov/models/ner_rus_bert/model\n"
     ]
    }
   ],
   "source": [
    "from natasha import DatesExtractor\n",
    "import numpy as np\n",
    "from deeppavlov import configs, build_model\n",
    "\n",
    "ner_model = build_model(configs.ner.ner_rus_bert, download=True)\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "segmenter = Segmenter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from razdel import tokenize, sentenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def preprocess_with_natasha(text: str) -> str:\n",
    "    text = str(text)\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_ner(ner_tagger)\n",
    "    for ner in doc.spans:\n",
    "        text = text.replace(ner.text, \"\")\n",
    "    return \" \".join(text.split())\n",
    "#     return my_preprocess(text)\n",
    "\n",
    "def preprocess_with_deepmipt(text: str) -> str:\n",
    "    text = str(text)\n",
    "    text_splitted = sentenize(text)\n",
    "    for sent in text_splitted:\n",
    "        sent = str(sent.text)\n",
    "        if len(sent) == 0:\n",
    "            break\n",
    "        sent_buf = list(tokenize(sent))\n",
    "        if len(sent_buf) >= 300:\n",
    "            sent = \" \".join([str(str_buf) for str_buf in sent_buf[:300]])\n",
    "#         print(len(sent))\n",
    "#         print(sent)\n",
    "        res = ner_model([sent])\n",
    "        for m in zip(res[0][0], res[1][0]):\n",
    "            if m[1][0] == \"B\" or m[1][0] == \"I\":\n",
    "                text = text.replace(m[0], \"\")\n",
    "            \n",
    "    return \" \".join(text.split())\n",
    "#     return my_preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Задача 2__:    \n",
    "На предыдущем занятии вы реализовывали функции поиска ближайших ответов на запросы через TF-IDF и BM25. \n",
    "Сравните качество нахождения верного ответа для обоих методов в трех случаях:\n",
    "- с функцией ```preprocess_with_natasha```\n",
    "- с функцией ```preprocess_with_deepmipt```\n",
    "- без препроцессинга\n",
    "\n",
    "Для измерения качества используйте метрику accuracy. Считаем, что ответ верный, если он входит в топ-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_df = pd.read_excel(\"answers_base.xlsx\")\n",
    "questions_df = pd.read_excel(\"queries_base.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natasha + TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Anna/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n",
      "100%|██████████| 1652/1652 [00:10<00:00, 152.83it/s]\n",
      "100%|██████████| 690/690 [00:04<00:00, 156.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1652, 13065)\n",
      "X_test.shape: (690, 13065)\n"
     ]
    }
   ],
   "source": [
    "answers_df.rename(columns={'Текст вопросов': 'text', 'Номер связки': 'join_num'}, inplace=True)\n",
    "questions_df.rename(columns={'Текст вопроса': 'text', 'Номер связки\\n': 'join_num'}, inplace=True)\n",
    "\n",
    "train, test_quest = train_test_split(questions_df, test_size=0.3)\n",
    "\n",
    "train_quest = pd.concat([answers_df, train])\n",
    "\n",
    "train_quest_processed = []\n",
    "test_quest_processed = []\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# preprocess train / test\n",
    "for quest in tqdm(train_quest['text']):\n",
    "    train_quest_processed.append(preprocess_with_natasha(quest))\n",
    "for quest in tqdm(test_quest['text']):\n",
    "    test_quest_processed.append(preprocess_with_natasha(quest))\n",
    "\n",
    "vectorizer.fit(train_quest_processed + test_quest_processed)\n",
    "# train matrix\n",
    "X_train = vectorizer.transform(train_quest_processed)\n",
    "# test natrix\n",
    "X_test = vectorizer.transform(test_quest_processed)\n",
    "\n",
    "print(\"X_train.shape: \" + str(X_train.shape))\n",
    "print(\"X_test.shape: \" + str(X_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy with natasha and TF-IDF: 0.5043478260869565'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating = X_train.dot(X_test.T).argmax(axis=0)\n",
    "rating = np.array(rating)[0]\n",
    "count = 0\n",
    "for ind_test, pred in enumerate(rating):\n",
    "    if math.isnan(test_quest.iloc[ind_test].join_num) or math.isnan(train_quest.iloc[pred].join_num):\n",
    "        continue\n",
    "\n",
    "    if int(test_quest.iloc[ind_test].join_num) == int(train_quest.iloc[pred].join_num):\n",
    "        count += 1\n",
    "\n",
    "\"Accuracy with natasha and TF-IDF: \" + str(count / len(rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepmipt + TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1652/1652 [12:07<00:00,  2.27it/s]\n",
      "100%|██████████| 690/690 [05:53<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1652, 13934)\n",
      "X_test.shape: (690, 13934)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Accuracy with deepmipt and TF-IDF: 0.518840579710145'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_quest_processed = []\n",
    "test_quest_processed = []\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# preprocess train / test\n",
    "for quest in tqdm(train_quest['text']):\n",
    "    train_quest_processed.append(preprocess_with_deepmipt(quest))\n",
    "for quest in tqdm(test_quest['text']):\n",
    "    test_quest_processed.append(preprocess_with_deepmipt(quest))\n",
    "\n",
    "vectorizer.fit(train_quest_processed + test_quest_processed)\n",
    "# train matrix\n",
    "X_train = vectorizer.transform(train_quest_processed)\n",
    "# test natrix\n",
    "X_test = vectorizer.transform(test_quest_processed)\n",
    "\n",
    "print(\"X_train.shape: \" + str(X_train.shape))\n",
    "print(\"X_test.shape: \" + str(X_test.shape))\n",
    "\n",
    "rating = X_train.dot(X_test.T).argmax(axis=0)\n",
    "rating = np.array(rating)[0]\n",
    "count = 0\n",
    "for ind_test, pred in enumerate(rating):\n",
    "    if math.isnan(test_quest.iloc[ind_test].join_num) or math.isnan(train_quest.iloc[pred].join_num):\n",
    "        continue\n",
    "\n",
    "    if int(test_quest.iloc[ind_test].join_num) == int(train_quest.iloc[pred].join_num):\n",
    "        count += 1\n",
    "\n",
    "\"Accuracy with deepmipt and TF-IDF: \" + str(count / len(rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without preprocessing + TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1652/1652 [00:00<00:00, 33803.42it/s]\n",
      "100%|██████████| 690/690 [00:00<00:00, 476939.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1652, 14972)\n",
      "X_test.shape: (690, 14972)\n"
     ]
    }
   ],
   "source": [
    "train_quest_processed = []\n",
    "test_quest_processed = []\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# preprocess train / test\n",
    "for quest in tqdm(train_quest['text']):\n",
    "    train_quest_processed.append(str(quest))\n",
    "for quest in tqdm(test_quest['text']):\n",
    "    test_quest_processed.append(str(quest))\n",
    "\n",
    "vectorizer.fit(train_quest_processed + test_quest_processed)\n",
    "# train matrix\n",
    "X_train = vectorizer.transform(train_quest_processed)\n",
    "# test natrix\n",
    "X_test = vectorizer.transform(test_quest_processed)\n",
    "\n",
    "print(\"X_train.shape: \" + str(X_train.shape))\n",
    "print(\"X_test.shape: \" + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy without any preprocessing and TF-IDF: 0.5318840579710145'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating = X_train.dot(X_test.T).argmax(axis=0)\n",
    "rating = np.array(rating)[0]\n",
    "count = 0\n",
    "for ind_test, pred in enumerate(rating):\n",
    "    if math.isnan(test_quest.iloc[ind_test].join_num) or math.isnan(train_quest.iloc[pred].join_num):\n",
    "        continue\n",
    "\n",
    "    if int(test_quest.iloc[ind_test].join_num) == int(train_quest.iloc[pred].join_num):\n",
    "        count += 1\n",
    "\n",
    "\"Accuracy without any preprocessing and TF-IDF: \" + str(count / len(rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 векторизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from razdel import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def get_inverse_dict(mat):\n",
    "    d = dict()\n",
    "    mat = mat.toarray()\n",
    "    for ind, word in enumerate(vectorizer.get_feature_names()):\n",
    "        d[word] = [int(sum(mat[:, ind]))]\n",
    "        for ind_j, doc_ind in enumerate(mat[:, ind].tolist()):\n",
    "            if doc_ind != 0:\n",
    "                d[word].append(doc_ind)\n",
    "        d[word].append(ind)\n",
    "    return d\n",
    "\n",
    "\n",
    "def vec_bm25(doc, dict_words):\n",
    "    vec = np.zeros((1, len(dict_words)))\n",
    "    doc = preprocess_with_natasha(doc)\n",
    "    for word in doc.split(\" \"):\n",
    "        if word in dict_words.keys():\n",
    "            vec[0, dict_words[word][-1]] = 1\n",
    "    return vec\n",
    "\n",
    "\n",
    "def bm25_vectorizer(tf_val, len_d, corpus_len, nq):\n",
    "    k = 2.0\n",
    "    b = 0.75\n",
    "    IDF = np.log((corpus_len-nq+0.5) / (nq+0.5))\n",
    "    TF = (tf_val * (k+1)) / (tf_val + k * (1-b+b*(len_d / avrdl)))\n",
    "    return TF * IDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without preprocessing + BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1652/1652 [00:00<00:00, 608500.06it/s]\n",
      "100%|██████████| 690/690 [00:00<00:00, 389878.72it/s]\n"
     ]
    }
   ],
   "source": [
    "train_quest_processed = []\n",
    "test_quest_processed = []\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# preprocess train / test\n",
    "for quest in tqdm(train_quest['text']):\n",
    "    train_quest_processed.append(str(quest))\n",
    "for quest in tqdm(test_quest['text']):\n",
    "    test_quest_processed.append(str(quest))\n",
    "\n",
    "corpus_len = len(train_quest_processed)\n",
    "avrdl = sum([len(i.split(\" \")) for i in train_quest_processed]) / corpus_len\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_quest_processed)\n",
    "\n",
    "inverse_dict = get_inverse_dict(X_train)\n",
    "\n",
    "mat = np.zeros((corpus_len, len(inverse_dict)))\n",
    "\n",
    "for ind, doc in enumerate(train_quest_processed):\n",
    "    tokens = doc.split(\" \")\n",
    "    tf_values = collections.Counter(tokens)\n",
    "    len_d = len(tokens)\n",
    "    for word in tokens:\n",
    "        if word not in inverse_dict.keys():\n",
    "            continue\n",
    "#         print(\"Ffff\")\n",
    "        mat[ind, inverse_dict[word][-1]] = bm25_vectorizer(tf_values[word],\n",
    "                                                         len_d,\n",
    "                                                         corpus_len,\n",
    "                                                         len(inverse_dict[word]) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy without any preprocessing and BM25: 0.5057971014492754'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vecs = []\n",
    "for i in test_quest.text.values:\n",
    "    test_vecs.append(vec_bm25(i, inverse_dict)[0])\n",
    "test_vecs = np.array(test_vecs)\n",
    "\n",
    "rating = mat.dot(test_vecs.T).argmax(axis=0)\n",
    "rating = np.array(rating)\n",
    "\n",
    "count = 0\n",
    "for ind_test, pred in enumerate(rating):\n",
    "    if math.isnan(test_quest.iloc[ind_test].join_num) or math.isnan(train_quest.iloc[pred].join_num):\n",
    "        continue\n",
    "\n",
    "    if int(test_quest.iloc[ind_test].join_num) == int(train_quest.iloc[pred].join_num):\n",
    "        count += 1\n",
    "\n",
    "\"Accuracy without any preprocessing and BM25: \" + str(count / len(rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natasha + BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1652/1652 [00:19<00:00, 86.93it/s] \n",
      "100%|██████████| 690/690 [00:05<00:00, 121.80it/s]\n"
     ]
    }
   ],
   "source": [
    "train_quest_processed = []\n",
    "test_quest_processed = []\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# preprocess train / test\n",
    "for quest in tqdm(train_quest['text']):\n",
    "    train_quest_processed.append(preprocess_with_natasha(quest))\n",
    "for quest in tqdm(test_quest['text']):\n",
    "    test_quest_processed.append(preprocess_with_natasha(quest))\n",
    "\n",
    "corpus_len = len(train_quest_processed)\n",
    "avrdl = sum([len(i.split(\" \")) for i in train_quest_processed]) / corpus_len\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_quest_processed)\n",
    "\n",
    "inverse_dict = get_inverse_dict(X_train)\n",
    "\n",
    "mat = np.zeros((corpus_len, len(inverse_dict)))\n",
    "\n",
    "for ind, doc in enumerate(train_quest_processed):\n",
    "    tokens = doc.split(\" \")\n",
    "    tf_values = collections.Counter(tokens)\n",
    "    len_d = len(tokens)\n",
    "    for word in tokens:\n",
    "        if word not in inverse_dict.keys():\n",
    "            continue\n",
    "#         print(\"Ffff\")\n",
    "        mat[ind, inverse_dict[word][-1]] = bm25_vectorizer(tf_values[word],\n",
    "                                                         len_d,\n",
    "                                                         corpus_len,\n",
    "                                                         len(inverse_dict[word]) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy with natasha and BM25: 0.508695652173913'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vecs = []\n",
    "for i in test_quest.text.values:\n",
    "    test_vecs.append(vec_bm25(i, inverse_dict)[0])\n",
    "test_vecs = np.array(test_vecs)\n",
    "\n",
    "rating = mat.dot(test_vecs.T).argmax(axis=0)\n",
    "rating = np.array(rating)\n",
    "\n",
    "count = 0\n",
    "for ind_test, pred in enumerate(rating):\n",
    "    if math.isnan(test_quest.iloc[ind_test].join_num) or math.isnan(train_quest.iloc[pred].join_num):\n",
    "        continue\n",
    "\n",
    "    if int(test_quest.iloc[ind_test].join_num) == int(train_quest.iloc[pred].join_num):\n",
    "        count += 1\n",
    "\n",
    "\"Accuracy with natasha and BM25: \" + str(count / len(rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepmipt + BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1652/1652 [10:42<00:00,  2.57it/s]  \n",
      "100%|██████████| 690/690 [06:56<00:00,  1.66it/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Accuracy with deepmipt and BM25: 0.5130434782608696'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_quest_processed = []\n",
    "test_quest_processed = []\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# preprocess train / test\n",
    "for quest in tqdm(train_quest['text']):\n",
    "    train_quest_processed.append(preprocess_with_deepmipt(quest))\n",
    "for quest in tqdm(test_quest['text']):\n",
    "    test_quest_processed.append(preprocess_with_deepmipt(quest))\n",
    "\n",
    "corpus_len = len(train_quest_processed)\n",
    "avrdl = sum([len(i.split(\" \")) for i in train_quest_processed]) / corpus_len\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_quest_processed)\n",
    "\n",
    "inverse_dict = get_inverse_dict(X_train)\n",
    "\n",
    "mat = np.zeros((corpus_len, len(inverse_dict)))\n",
    "\n",
    "for ind, doc in enumerate(train_quest_processed):\n",
    "    tokens = doc.split(\" \")\n",
    "    tf_values = collections.Counter(tokens)\n",
    "    len_d = len(tokens)\n",
    "    for word in tokens:\n",
    "        if word not in inverse_dict.keys():\n",
    "            continue\n",
    "#         print(\"Ffff\")\n",
    "        mat[ind, inverse_dict[word][-1]] = bm25_vectorizer(tf_values[word],\n",
    "                                                         len_d,\n",
    "                                                         corpus_len,\n",
    "                                                         len(inverse_dict[word]) - 1)\n",
    "    \n",
    "test_vecs = []\n",
    "for i in test_quest.text.values:\n",
    "    test_vecs.append(vec_bm25(i, inverse_dict)[0])\n",
    "test_vecs = np.array(test_vecs)\n",
    "\n",
    "rating = mat.dot(test_vecs.T).argmax(axis=0)\n",
    "rating = np.array(rating)\n",
    "\n",
    "count = 0\n",
    "for ind_test, pred in enumerate(rating):\n",
    "    if math.isnan(test_quest.iloc[ind_test].join_num) or math.isnan(train_quest.iloc[pred].join_num):\n",
    "        continue\n",
    "\n",
    "    if int(test_quest.iloc[ind_test].join_num) == int(train_quest.iloc[pred].join_num):\n",
    "        count += 1\n",
    "\n",
    "\"Accuracy with deepmipt and BM25: \" + str(count / len(rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Задача 3__:    \n",
    "Улучшить правила в natasha. Написать правила, которые ловят даты в следующих примерах и пересчитать статистику из Задачи 2:\n",
    "- Уехал 8-9 ноября в Сочи\n",
    "- Уезжаю 5 числа                           \n",
    "- 20го сентября заболел\n",
    "\n",
    "Пример можно посмотреть тут: https://github.com/natasha/yargy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 19) ['8', 'января', '2014', 'года']\n",
      "[21, 36) ['15', 'июня', '2001', 'г', '.']\n",
      "[38, 53) ['31', 'февраля', '2018']\n",
      "[60, 70) ['8', '-', '9', 'ноября']\n",
      "[85, 92) ['5', 'числа']\n",
      "[93, 106) ['20', 'го', 'сентября']\n"
     ]
    }
   ],
   "source": [
    "from yargy import or_\n",
    "from yargy.predicates import caseless, normalized, dictionary\n",
    "from yargy import rule, and_, Parser\n",
    "from yargy.predicates import gte, lte\n",
    "\n",
    "\n",
    "DAY = and_(\n",
    "    gte(1),\n",
    "    lte(31)\n",
    ")\n",
    "MONTH = and_(\n",
    "    gte(1),\n",
    "    lte(12)\n",
    ")\n",
    "YEAR = and_(\n",
    "    gte(1),\n",
    "    lte(2018)\n",
    ")\n",
    "DATE = rule(\n",
    "    YEAR,\n",
    "    '-',\n",
    "    MONTH,\n",
    "    '-',\n",
    "    DAY\n",
    ")\n",
    "\n",
    "\n",
    "MONTHS = {\n",
    "    'январь',\n",
    "    'февраль',\n",
    "    'март',\n",
    "    'апрель',\n",
    "    'мая',\n",
    "    'июнь',\n",
    "    'июль',\n",
    "    'август',\n",
    "    'сентябрь',\n",
    "    'октябрь',\n",
    "    'ноябрь',\n",
    "    'декабрь'\n",
    "}\n",
    "MONTH_NAME = dictionary(MONTHS)\n",
    "YEAR_WORDS = or_(\n",
    "    rule(caseless('г'), '.'),\n",
    "    rule(normalized('год')),\n",
    "    rule(normalized('число')),\n",
    "    rule(caseless('числа')),\n",
    "    rule(caseless('го'))\n",
    ")\n",
    "\n",
    "DATE = or_(\n",
    "    rule(\n",
    "        YEAR,\n",
    "        '-',\n",
    "        MONTH,\n",
    "        '-',\n",
    "        DAY\n",
    "    ),\n",
    "    rule(\n",
    "        DAY,\n",
    "        MONTH_NAME,\n",
    "        YEAR,\n",
    "        YEAR_WORDS.optional()\n",
    "    ),\n",
    "    rule(\n",
    "        DAY,\n",
    "        \"-\",\n",
    "        DAY,\n",
    "        MONTH_NAME,\n",
    "    ),\n",
    "    rule(\n",
    "        DAY,\n",
    "        YEAR_WORDS.optional(),\n",
    "        MONTH_NAME.optional()\n",
    "    )\n",
    ")\n",
    "\n",
    "parser = Parser(DATE)\n",
    "text = '''\n",
    "8 января 2014 года, 15 июня 2001 г.,\n",
    "31 февраля 2018\n",
    "Уехал 8-9 ноября в Сочи\n",
    "Уезжаю 5 числа\n",
    "20го сентября заболел'''\n",
    "for match in parser.findall(text):\n",
    "    print(match.span, [_.value for _ in match.tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser(DATE)\n",
    "\n",
    "def preprocess_with_natasha_date(text: str) -> str:\n",
    "    text = str(text)\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_ner(ner_tagger)\n",
    "    for ner in doc.spans:\n",
    "        text = text.replace(ner.text, \"\")\n",
    "    for match in parser.findall(text):\n",
    "        for tok in match.tokens:\n",
    "            text = text.replace(tok.value, \"\")\n",
    "    return \" \".join(text.split())\n",
    "#     return my_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Anna/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n",
      "100%|██████████| 1652/1652 [00:26<00:00, 61.76it/s] \n",
      "100%|██████████| 690/690 [00:08<00:00, 82.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1652, 12945)\n",
      "X_test.shape: (690, 12945)\n"
     ]
    }
   ],
   "source": [
    "answers_df.rename(columns={'Текст вопросов': 'text', 'Номер связки': 'join_num'}, inplace=True)\n",
    "questions_df.rename(columns={'Текст вопроса': 'text', 'Номер связки\\n': 'join_num'}, inplace=True)\n",
    "\n",
    "train, test_quest = train_test_split(questions_df, test_size=0.3)\n",
    "\n",
    "train_quest = pd.concat([answers_df, train])\n",
    "\n",
    "train_quest_processed = []\n",
    "test_quest_processed = []\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# preprocess train / test\n",
    "for quest in tqdm(train_quest['text']):\n",
    "    train_quest_processed.append(preprocess_with_natasha_date(quest))\n",
    "for quest in tqdm(test_quest['text']):\n",
    "    test_quest_processed.append(preprocess_with_natasha_date(quest))\n",
    "\n",
    "vectorizer.fit(train_quest_processed + test_quest_processed)\n",
    "# train matrix\n",
    "X_train = vectorizer.transform(train_quest_processed)\n",
    "# test natrix\n",
    "X_test = vectorizer.transform(test_quest_processed)\n",
    "\n",
    "print(\"X_train.shape: \" + str(X_train.shape))\n",
    "print(\"X_test.shape: \" + str(X_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy with natasha and TF-IDF: 0.5391304347826087'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating = X_train.dot(X_test.T).argmax(axis=0)\n",
    "rating = np.array(rating)[0]\n",
    "count = 0\n",
    "for ind_test, pred in enumerate(rating):\n",
    "    if math.isnan(test_quest.iloc[ind_test].join_num) or math.isnan(train_quest.iloc[pred].join_num):\n",
    "        continue\n",
    "\n",
    "    if int(test_quest.iloc[ind_test].join_num) == int(train_quest.iloc[pred].join_num):\n",
    "        count += 1\n",
    "\n",
    "\"Accuracy with natasha and TF-IDF: \" + str(count / len(rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
